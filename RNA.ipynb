{"cells":[{"cell_type":"code","source":"import pandas as pd\nimport nltk\nimport re\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom bs4 import BeautifulSoup\nimport numpy as np\nimport tensorflow\nfrom tensorflow.keras.preprocessing import text_dataset_from_directory\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.layers import TextVectorization, Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense\nfrom tensorflow.keras.models import load_model\nfrom sklearn.model_selection import train_test_split\n\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\nstop_words = set(stopwords.words(\"english\"))\nlemmatizer = WordNetLemmatizer()\n\ndef strip_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()\n\ndef clean_text(text):\n    text = strip_html(text)\n    text = re.sub(r'[^A-Za-z0-9]+',' ',text)\n    text = text.lower()\n    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n    text = [word for word in text if not word in stop_words]\n    text = \" \".join(text)\n    return text\n\ndf = pd.read_csv('IMDB Dataset.csv', encoding = 'Latin-1')\ndf['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\ndf['Processed_Reviews'] = df.review.apply(lambda x: clean_text(x))\n\nx = df['Processed_Reviews']\ny = df['sentiment']\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n\nfrom_scratch = True \n\nvector_layer = TextVectorization(standardize='lower_and_strip_punctuation',\n                                 max_tokens=10000,\n                                 output_mode='int',\n                                 output_sequence_length=30)\nvector_layer.adapt(np.array(X_train))\n\n# Save the TextVectorization layer's vocabulary\nvocab = vector_layer.get_vocabulary()\nwith open('vocab.txt', 'w') as f:\n    for item in vocab:\n        f.write(\"%s\\n\" % item)\n\nif from_scratch:\n    model = Sequential()\n    model.add(Input(shape=(1,), dtype=tensorflow.string))\n    model.add(vector_layer)\n    model.add(Embedding(10001, 16))\n    model.add(Dropout(0.2))\n    model.add(Conv1D(128, 5, activation='relu'))\n    model.add(GlobalMaxPooling1D())\n    model.add(Dropout(0.2))\n    model.add(Dense(1, activation='sigmoid'))\nelse:\n    model = load_model('sentiment')\n\n    # Load the vocabulary\n    with open('vocab.txt') as f:\n        vocab = [line.rstrip() for line in f]\n\n    # Create a new TextVectorization layer\n    new_vector_layer = TextVectorization(standardize='lower_and_strip_punctuation',\n                                         max_tokens=10000,\n                                         output_mode='int',\n                                         output_sequence_length=30)\n    new_vector_layer.set_vocabulary(vocab)\n\n    # Replace the old vectorization layer with the new one\n    model.layers[1] = new_vector_layer\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(np.array(X_train), np.array(y_train), validation_data=(np.array(X_test), np.array(y_test)), epochs=20)\n\nmodel.save('sentiment')\n\n_, accuracy = model.evaluate(np.array(X_test), np.array(y_test))\nprint(accuracy)\n","metadata":{"id":"tjQkiLgU2NKN","colab":{"height":467,"base_uri":"https://localhost:8080/"},"cell_id":"1b602d87a1fe406c9f8b9e9cd839b705","outputId":"7b016285-e184-432c-cb32-49bd1a9978df","source_hash":"ee5d7b46","execution_start":1685929180900,"execution_millis":448946,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"2023-06-05 01:39:43.213599: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-06-05 01:39:43.368640: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2023-06-05 01:39:43.368671: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2023-06-05 01:39:43.404767: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2023-06-05 01:39:44.407599: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2023-06-05 01:39:44.407734: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2023-06-05 01:39:44.407751: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n[nltk_data] Downloading package wordnet to /root/nltk_data...\n[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n/shared-libs/python3.9/py-core/lib/python3.9/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n  warnings.warn(\n2023-06-05 01:41:40.641308: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n2023-06-05 01:41:40.641343: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n2023-06-05 01:41:40.641368: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (p-7a5b1e8f-5fb8-49ad-8f36-77068147d699): /proc/driver/nvidia/version does not exist\n2023-06-05 01:41:40.641720: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\nEpoch 1/20\n1250/1250 [==============================] - 16s 13ms/step - loss: 0.5215 - accuracy: 0.7347 - val_loss: 0.4657 - val_accuracy: 0.7739\nEpoch 2/20\n1250/1250 [==============================] - 15s 12ms/step - loss: 0.4053 - accuracy: 0.8167 - val_loss: 0.4660 - val_accuracy: 0.7752\nEpoch 3/20\n1250/1250 [==============================] - 16s 13ms/step - loss: 0.3469 - accuracy: 0.8481 - val_loss: 0.4870 - val_accuracy: 0.7750\nEpoch 4/20\n1250/1250 [==============================] - 16s 13ms/step - loss: 0.2997 - accuracy: 0.8748 - val_loss: 0.5191 - val_accuracy: 0.7715\nEpoch 5/20\n1250/1250 [==============================] - 15s 12ms/step - loss: 0.2597 - accuracy: 0.8941 - val_loss: 0.5607 - val_accuracy: 0.7698\nEpoch 6/20\n1250/1250 [==============================] - 15s 12ms/step - loss: 0.2242 - accuracy: 0.9089 - val_loss: 0.6213 - val_accuracy: 0.7655\nEpoch 7/20\n1250/1250 [==============================] - 16s 13ms/step - loss: 0.1930 - accuracy: 0.9236 - val_loss: 0.6752 - val_accuracy: 0.7596\nEpoch 8/20\n1250/1250 [==============================] - 15s 12ms/step - loss: 0.1679 - accuracy: 0.9350 - val_loss: 0.7368 - val_accuracy: 0.7580\nEpoch 9/20\n1250/1250 [==============================] - 15s 12ms/step - loss: 0.1519 - accuracy: 0.9405 - val_loss: 0.7898 - val_accuracy: 0.7580\nEpoch 10/20\n1250/1250 [==============================] - 15s 12ms/step - loss: 0.1365 - accuracy: 0.9474 - val_loss: 0.8412 - val_accuracy: 0.7524\nEpoch 11/20\n1250/1250 [==============================] - 15s 12ms/step - loss: 0.1186 - accuracy: 0.9545 - val_loss: 0.9189 - val_accuracy: 0.7537\nEpoch 12/20\n1250/1250 [==============================] - 16s 13ms/step - loss: 0.1163 - accuracy: 0.9554 - val_loss: 0.9295 - val_accuracy: 0.7496\nEpoch 13/20\n1250/1250 [==============================] - 17s 14ms/step - loss: 0.1024 - accuracy: 0.9606 - val_loss: 0.9857 - val_accuracy: 0.7468\nEpoch 14/20\n1250/1250 [==============================] - 17s 14ms/step - loss: 0.0926 - accuracy: 0.9646 - val_loss: 1.0609 - val_accuracy: 0.7458\nEpoch 15/20\n1250/1250 [==============================] - 17s 14ms/step - loss: 0.0903 - accuracy: 0.9656 - val_loss: 1.0695 - val_accuracy: 0.7481\nEpoch 16/20\n1250/1250 [==============================] - 17s 14ms/step - loss: 0.0853 - accuracy: 0.9673 - val_loss: 1.1234 - val_accuracy: 0.7503\nEpoch 17/20\n1250/1250 [==============================] - 17s 13ms/step - loss: 0.0795 - accuracy: 0.9699 - val_loss: 1.1498 - val_accuracy: 0.7464\nEpoch 18/20\n1250/1250 [==============================] - 16s 13ms/step - loss: 0.0754 - accuracy: 0.9721 - val_loss: 1.1653 - val_accuracy: 0.7483\nEpoch 19/20\n1250/1250 [==============================] - 16s 13ms/step - loss: 0.0697 - accuracy: 0.9737 - val_loss: 1.2299 - val_accuracy: 0.7483\nEpoch 20/20\n1250/1250 [==============================] - 17s 13ms/step - loss: 0.0665 - accuracy: 0.9751 - val_loss: 1.2310 - val_accuracy: 0.7422\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: sentiment/assets\nINFO:tensorflow:Assets written to: sentiment/assets\n313/313 [==============================] - 1s 4ms/step - loss: 1.2310 - accuracy: 0.7422\n0.7422000169754028\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"predictions =model.predict([\"The movie was generally bad, the plot was boring and the characters badly interpreted\"])\nprediction = float(predictions[0][0])\nclass_name = 'negative' if prediction < 0.5 else 'positive'\nif class_name == 'positive':\n    confidence = (prediction - 0.5) / 0.5\nelse:\n    confidence = (0.5 - prediction) / 0.5\nprint({'sentiment': class_name, 'confidence': confidence})","metadata":{"id":"8q86Jaz36_J2","cell_id":"6ec3c32f45e545e3bd49067599062cf4","source_hash":"4c3126ab","execution_start":1685929629844,"execution_millis":232,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 171ms/step\n{'sentiment': 'negative', 'confidence': 0.8420833647251129}\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install keras-tuner","metadata":{"id":"TSEbmFnFBCVm","colab":{"base_uri":"https://localhost:8080/"},"cell_id":"7fe43b29741345b3a9b29407e795e9fc","outputId":"ec50fcb4-7d7e-47d7-90f7-f4f3e2e05a94","source_hash":"564a00c6","execution_start":1685929630080,"execution_millis":3901,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Collecting keras-tuner\n  Downloading keras_tuner-1.3.5-py3-none-any.whl (176 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.1/176.1 KB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from keras-tuner) (21.3)\nCollecting kt-legacy\n  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\nRequirement already satisfied: requests in /shared-libs/python3.9/py/lib/python3.9/site-packages (from keras-tuner) (2.28.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from packaging->keras-tuner) (3.0.9)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests->keras-tuner) (1.26.12)\nRequirement already satisfied: certifi>=2017.4.17 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests->keras-tuner) (2022.9.24)\nRequirement already satisfied: idna<4,>=2.5 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests->keras-tuner) (3.4)\nRequirement already satisfied: charset-normalizer<3,>=2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests->keras-tuner) (2.1.1)\nInstalling collected packages: kt-legacy, keras-tuner\nSuccessfully installed keras-tuner-1.3.5 kt-legacy-1.0.5\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport nltk\nimport re\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom bs4 import BeautifulSoup\nimport numpy as np\nimport tensorflow\nfrom tensorflow.keras.layers import TextVectorization, Embedding, Bidirectional, LSTM, Dropout, Dense\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras import Input\nfrom sklearn.model_selection import train_test_split\nfrom kerastuner.tuners import RandomSearch\n\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\nstop_words = set(stopwords.words(\"english\"))\nlemmatizer = WordNetLemmatizer()\n\ndef strip_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()\n\ndef clean_text(text):\n    text = strip_html(text)\n    text = re.sub(r'[^A-Za-z0-9]+',' ',text)\n    text = text.lower()\n    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n    text = [word for word in text if not word in stop_words]\n    text = \" \".join(text)\n    return text\n\ndf = pd.read_csv('IMDB Dataset.csv', encoding = 'Latin-1')\ndf['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\ndf['Processed_Reviews'] = df.review.apply(lambda x: clean_text(x))\n\nx = df['Processed_Reviews']\ny = df['sentiment']\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n\nvector_layer = TextVectorization(standardize='lower_and_strip_punctuation',\n                                 max_tokens=10000,\n                                 output_mode='int',\n                                 output_sequence_length=100)\nvector_layer.adapt(np.array(X_train))\n\nvocab = vector_layer.get_vocabulary()\nwith open('vocab.txt', 'w') as f:\n    for item in vocab:\n        f.write(\"%s\\n\" % item)\n\nfrom_scratch = True \n\nif from_scratch:\n    def build_model(hp):\n        model = Sequential()\n        model.add(Input(shape=(1,), dtype=tensorflow.string))\n        model.add(vector_layer)\n        model.add(Embedding(10000, hp.Int('embedding_dim', min_value=32, max_value=512, step=32)))\n        model.add(Bidirectional(LSTM(hp.Int('LSTM_units', min_value=32, max_value=512, step=32))))\n        model.add(Dropout(hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1)))\n        model.add(Dense(1, activation='sigmoid'))\n        \n        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n        \n        return model\n\n    tuner = RandomSearch(\n        build_model,\n        objective='val_accuracy',\n        max_trials=3,\n        executions_per_trial=3,\n        directory='model_dir',\n        project_name='sentiment_analysis'\n    )\n\n    tuner.search(np.array(X_train), np.array(y_train), epochs=3, validation_data=(np.array(X_test), np.array(y_test)))\n\n    best_model = tuner.get_best_models(num_models=1)[0]\nelse:\n    best_model = load_model('sentiment')\n\n    with open('vocab.txt') as f:\n        vocab = [line.rstrip() for line in f]\n\n    new_vector_layer = TextVectorization(standardize='lower_and_strip_punctuation',\n                                         max_tokens=10000,\n                                         output_mode='int',\n                                         output_sequence_length=100)\n    new_vector_layer.set_vocabulary(vocab)\n\n    best_model.layers[1] = new_vector_layer\n\nbest_model.fit(np.array(X_train), np.array(y_train), epochs=10, validation_data=(np.array(X_test), np.array(y_test)))\n\n# Save the model\nbest_model.save('sentiment')\n\n_, accuracy = best_model.evaluate(np.array(X_test), np.array(y_test))\nprint(accuracy)\n","metadata":{"id":"Muc9KYb9ASPg","colab":{"base_uri":"https://localhost:8080/"},"cell_id":"17117da079244ff78b0d5942ccf28fe9","outputId":"848c506f-baf8-4965-b82e-eb2d3fc13e61","source_hash":"ffdcef70","execution_start":1685929634002,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Trial 3 Complete [02h 24m 30s]\nval_accuracy: 0.8592666784922282\n\nBest val_accuracy So Far: 0.8646666606267294\nTotal elapsed time: 06h 22m 46s\nINFO:tensorflow:Oracle triggered exit\nINFO:tensorflow:Oracle triggered exit\nEpoch 1/10\n1250/1250 [==============================] - 1326s 1s/step - loss: 0.1649 - accuracy: 0.9373 - val_loss: 0.3820 - val_accuracy: 0.8511\nEpoch 2/10\n1250/1250 [==============================] - 1229s 984ms/step - loss: 0.0962 - accuracy: 0.9658 - val_loss: 0.4721 - val_accuracy: 0.8533\nEpoch 3/10\n1250/1250 [==============================] - 1307s 1s/step - loss: 0.0610 - accuracy: 0.9799 - val_loss: 0.5654 - val_accuracy: 0.8444\nEpoch 4/10\n1250/1250 [==============================] - 1246s 996ms/step - loss: 0.0379 - accuracy: 0.9888 - val_loss: 0.6413 - val_accuracy: 0.8368\nEpoch 5/10\n 665/1250 [==============>...............] - ETA: 9:02 - loss: 0.0238 - accuracy: 0.9920","output_type":"stream"},{"output_type":"error","ename":"KernelInterrupted","evalue":"Execution interrupted by the Jupyter kernel.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKernelInterrupted\u001b[0m: Execution interrupted by the Jupyter kernel."]}],"execution_count":null},{"cell_type":"code","source":"predictions =best_model.predict([\"Quite horrible\"])\nprediction = float(predictions[0][0])\nclass_name = 'negative' if prediction < 0.5 else 'positive'\nif class_name == 'positive':\n    confidence = (prediction - 0.5) / 0.5\nelse:\n    confidence = (0.5 - prediction) / 0.5\nprint({'sentiment': class_name, 'confidence': confidence})","metadata":{"id":"7-8D-4CYAUc6","colab":{"base_uri":"https://localhost:8080/"},"cell_id":"9ddfe4eeeeb14f80b91487b61882033e","outputId":"5fc50a02-f4a7-49f9-efa4-d7702b7a2715","source_hash":"9b3b5bcd","deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions =best_model.predict([\"The movie was generally bad, the plot was boring and the characters badly interpreted\"])\nprediction = float(predictions[0][0])\nclass_name = 'negative' if prediction < 0.5 else 'positive'\nif class_name == 'positive':\n    confidence = (prediction - 0.5) / 0.5\nelse:\n    confidence = (0.5 - prediction) / 0.5\nprint({'sentiment': class_name, 'confidence': confidence})","metadata":{"id":"IBi-aidXHGzx","colab":{"base_uri":"https://localhost:8080/"},"cell_id":"77f67ba6f205426bb8536971c4b8b442","outputId":"d7eb37a7-0b42-4250-e18b-617325ba290d","source_hash":"30925969","deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save()","metadata":{"cell_id":"0902bb3d331341109de5aac5b9b303f1","source_hash":"ea96f0e2","deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=7a5b1e8f-5fb8-49ad-8f36-77068147d699' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"gpuType":"T4","provenance":[]},"deepnote":{},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","language_info":{"name":"python"},"deepnote_notebook_id":"fc6ed50f002649609dd16070ea05d812","deepnote_execution_queue":[]}}